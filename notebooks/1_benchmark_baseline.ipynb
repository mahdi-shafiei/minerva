{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from minerva.bayesopt import BayesianOptimisation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 1: Running a Bayesian optimisation benchmark on a virtual benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device, use gpu if available\n",
    "tkwargs = {\n",
    "        \"dtype\": torch.double,\n",
    "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This tutorial shows how to run a Bayesian optimisation benchmark on the emulated virtual benchmark datasets in the manuscript. The datasets are constructed as tables of concatenated descriptor representations and objective values**\n",
    "\n",
    "First, we show an example of what an input benchmark dataset looks like\n",
    "- The benchmark dataset is a table of rows containing the featurised representation of the reaction conditions and their corresponding objective values, in this case yield and turnover\n",
    "- The input features and target objectives are **not assumed to have undergone any scalarisation**\n",
    "- No other columns besides the input features and objective columns are assumed to be present\n",
    "- **Maximisation is assumed**, so minimisation objectives will have to be adjusted to their negative values\n",
    "- In this case, the reaction conditions consist of choice of ligand, which is one-hot encoded, with continuous variables residence time, reaction temperature, and catalyst loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suzuki_i_benchmark = pd.read_csv('../benchmark_datasets/olympus_suzuki/suzuki_i.csv', index_col=0)\n",
    "suzuki_i_benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the settings that we wish to run the Bayesian Optimisation benchmark on. \n",
    "\n",
    "| Arguments | Explanation |\n",
    "| --- | --- |\n",
    "| `seed` (int) | Random seed set using pytorch lightning |\n",
    "| `objective_columns` (List[str]) | List of strings denoting objective columns in the benchmark dataset |\n",
    "| `benchmark_path` (str) | File path to read the benchmark dataset file, example shown above <br> The virtual benchmark datasets used in the manuscript are included in this repository under minerva/benchmark_datasets |\n",
    "| `init_samples` (int) | Number of quasi-random Sobol samples to initialise <br> the optimisation campaign as initial training data |\n",
    "| `batch_size` (int) | Number of experiments to suggest in parallel per iteration |\n",
    "| `n_iterations` (int) | Number of iterations to run Bayesian optimisation for, <br> excluding the quasi-random initialisation |\n",
    "| `search_strategy` (str) | Acquisition function to use.  <br> Available choices are `qNEHVI`, `qNParEgo`, and `TS-HVI` |\n",
    "| `kernel` (str) | Kernel hyperparameters to use for Gaussian Process with a Matern Kernel. <br> Available choices are `default` and `edboplus` |\n",
    "| `noise_std` (float) | Level of noise standard deviation for Gaussian noise used to perturb the first objective value. <br> Since for our benchmark datasets this defaults to yield, they are clamped at `[0, 100]` in this implementation |\n",
    "| `noise_std_2` (float) | Level of noise standard deviation for Gaussian noise used to perturb the second objective value. <br> For our benchmark datasets, this is turnover, and is clamped at `0` and the max value for this work |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "objective_columns = ['yield', 'turnover'] # defining objective columns in the dataframe to be read and optimised in the benchmark dataframe\n",
    "benchmark_path = '../benchmark_datasets/olympus_suzuki/suzuki_i.csv' # in the same dataframe format as shown above\n",
    "\n",
    "init_samples = 24 \n",
    "batch_size = 24 \n",
    "n_iterations = 4 # iterations of BO in addition to initialisation we would like to run, total 24 + 24*4 = 120 in this case\n",
    "search_strategy = 'qNEHVI' \n",
    "kernel = 'edboplus' # choose kernel hyperparameters used in edboplus\n",
    "noise_std = 0\n",
    "noise_std_2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation of BO object\n",
    "Benchmark = BayesianOptimisation(device=tkwargs,seed=seed)\n",
    "Benchmark.load_and_preprocess_benchmark(data_df_path=benchmark_path, objective_columns=objective_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Bayesian optimisation**\n",
    "- Output metrics are displayed as hypervolume per iteration of optimisation from iteration 0 (sobol) to iteration n of the Bayesian optimisation. \n",
    "- Reference max hypervolumes of the existing dataset are also displayed\n",
    "- IGD+ and IGD metrics are also calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Benchmark.run_baseline_benchmark(\n",
    "    init_samples=init_samples,\n",
    "    batch_size=batch_size,\n",
    "    n_iterations=n_iterations,\n",
    "    search_strategy=search_strategy,\n",
    "    kernel='edboplus',\n",
    "    noise_std=noise_std,\n",
    "    noise_std_2=noise_std_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b02a12b6e09d04984cc20cb4777fb86fe654b18631361ce47d6857a6f00214fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
